{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-821c11149d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOnlyEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mt5_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#t5_model.encoder = STOnlyEncoder(st_model, st_tokenizer=st_tokenizer, t5_tokenizer=t5_tokenizer) #Unfortunately all these parameters are necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'encoder'"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModel, AutoTokenizer\n",
    "from experimental.t5_dataset import T5Dataset\n",
    "import json\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "st_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\") #model_args={\"embedding_size\":512})\n",
    "st_model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")#embedding_size=512)\n",
    "\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", extra_ids=1)\n",
    "\n",
    "\n",
    "########Building Frankenstein's monster #########\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "from experimental.test_t5 import STOnlyEncoder, CustomEncoder\n",
    "t5_model.encoder = CustomEncoder(t5_model.encoder)\n",
    "#t5_model.encoder = STOnlyEncoder(st_model, st_tokenizer=st_tokenizer, t5_tokenizer=t5_tokenizer) #Unfortunately all these parameters are necessary\n",
    "\n",
    "######### Monster building complete ##########\n",
    "\n",
    "#ORIGINAL_t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "DATA_DIR = \"DialogSum_Data\"\n",
    "\n",
    "SAMPLE_DATA = op.join(DATA_DIR, 'dialogsum.sample.jsonl')\n",
    "TEST_DATA = op.join(DATA_DIR, 'dialogsum.test.jsonl')\n",
    "TRAIN_DATA = op.join(DATA_DIR, 'dialogsum.train.jsonl')\n",
    "DEV_DATA = op.join(DATA_DIR, 'dialogsum.dev.jsonl')\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    \n",
    "    output_list = []\n",
    "    \n",
    "    with open(filepath) as sd_file:\n",
    "        lines = sd_file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        output_list.append(json.loads(line))\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dev_data_list = load_jsonl(DEV_DATA)\n",
    "    dev_dataset = T5Dataset(dev_data_list, \n",
    "                  tokenizer=t5_tokenizer,\n",
    "                  debug=True)   \n",
    "    for i in dev_dataset:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178])\n",
      "torch.Size([178])\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n",
      "n.sents: 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'encoder_extended_attention_mask' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ead19841b9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt5_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0;31m# greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m    991\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   1293\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1609\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0mencoder_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mencoder_extended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mencoder_extended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36minvert_attention_mask\u001b[0;34m(self, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# encoder_extended_attention_mask = (encoder_extended_attention_mask ==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# encoder_extended_attention_mask.transpose(-1, -2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mencoder_extended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fp16 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'encoder_extended_attention_mask' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "dev_dataset = T5Dataset(dev_data_list, \n",
    "                tokenizer=t5_tokenizer,\n",
    "                debug=True)   \n",
    "\n",
    "for i in dev_dataset:\n",
    "    ids = i[\"source_ids\"]\n",
    "    mask = i[\"source_mask\"]\n",
    "   \n",
    "    # print(\"#\"*20)\n",
    "    # print(ids)\n",
    "    # print(mask)\n",
    "    # print(dir(mask))\n",
    "    # print(dir(t5_model))\n",
    "   # print(type(ids))\n",
    "   # print(type(mask))\n",
    "    ORIG_encoding = t5_tokenizer.batch_encode_plus(\"some sentence here\", padding=True, return_tensors=\"pt\")\n",
    "    ORIG_ids = ORIG_encoding[\"input_ids\"]\n",
    "    ORIG_mask = ORIG_encoding[\"attention_mask\"]\n",
    "    output = t5_model.generate(input_ids = ORIG_ids, attention_mask = ORIG_mask)\n",
    "    # print(ORIG_output)\n",
    "    # print(\"o\"*10)\n",
    "    # print(ORIG_ids.shape)\n",
    "    print(ids.shape)\n",
    "    # print(\"f\"*10)\n",
    "    # print(ORIG_mask.shape)\n",
    "    print(mask.shape)\n",
    "\n",
    "    # print()\n",
    "    # print(ORIG_encoding)\n",
    "    # break\n",
    "    \n",
    "    output = t5_model.generate(input_ids=ids, attention_mask = mask)\n",
    "    print(t5_tokenizer.decode(output))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21b5f0ea4c5d1d5452f540f8b826290a2ddfb797a39b03c535ea5a8419c7448b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dialogsum-cdt-challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
