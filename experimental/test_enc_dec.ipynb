{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at patrickvonplaten/bert2bert-cnn_dailymail-fp16 were not used when initializing EncoderDecoderModel: ['decoder.bert.pooler.dense.weight', 'decoder.bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing EncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at patrickvonplaten/bert2bert-cnn_dailymail-fp16 were not used when initializing EncoderDecoderModel: ['decoder.bert.pooler.dense.weight', 'decoder.bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing EncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Module\n",
    "from transformers import EncoderDecoderModel, AutoTokenizer\n",
    "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", pad_token_id=0)\n",
    "orig_model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", pad_token_id=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 6251,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.batch_encode_plus([\"This is a sentence\"], padding=True, return_tensors=\"pt\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26515b729950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustomEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "import torch\n",
    "\n",
    "class Config():\n",
    "    pass\n",
    "\n",
    "class CustomEncoder(Module):\n",
    "    def __init__(self, encoder, *args, **kwargs):    \n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sub_encoder = encoder\n",
    "        pooler_config = Config()\n",
    "        pooler_config.hidden_size = 768\n",
    "        self.bert_pooler = BertPooler(pooler_config)\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "\n",
    "        attention_masks = kwargs.pop(\"attention_mask\")\n",
    "\n",
    "        print(attention_masks)\n",
    "\n",
    "        hidden_state_outputs = []\n",
    "        print(len(input_ids))\n",
    "        for i in range(len(input_ids)):\n",
    "            inputs = torch.unsqueeze(input_ids[i],0)\n",
    "            attn_mask = torch.ones_like(inputs)\n",
    "            encoder_outputs = self.sub_encoder.forward(inputs, attention_mask=attn_mask, **kwargs)\n",
    "            hidden_state_outputs.append(encoder_outputs[\"pooler_output\"])\n",
    "        hidden_state_outputs = pad_sequence(hidden_state_outputs).contiguous()\n",
    "       \n",
    "        \n",
    "        pooler_output = self.bert_pooler(hidden_state_outputs).contiguous()\n",
    "\n",
    "        output = BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=hidden_state_outputs,\n",
    "                                                              pooler_output = pooler_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "#class CustomDecoder(Module):\n",
    "#    def __init__(self, decoder, *args, **kwargs):\n",
    "#        super().__init__(*args, **kwargs)\n",
    "#        self.decoder = decoder\n",
    "\n",
    "#    def forward(self, input_ids, **kwargs):\n",
    "#        return self.decoder.forward(input_ids, **kwargs)[:,0]\n",
    "    \n",
    "#    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "#        print(args)\n",
    "#        print(kwargs)\n",
    "#        return self.decoder.prepare_inputs_for_generation(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dialogue = [\"this is a sentence\", \"what is the sentence about\", \"yes it is\"]\n",
    "tokens = tokenizer.batch_encode_plus(dialogue, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "cust_enc = CustomEncoder(model.encoder)\n",
    "#cust_dec = CustomDecoder(model.decoder)\n",
    "\n",
    "model.encoder = cust_enc\n",
    "#model.decoder = cust_dec\n",
    "\n",
    "tokens.pop(\"attention_mask\")\n",
    "tokens[\"attention_mask\"] = torch.Tensor([[1,1,1]])\n",
    "tokens.pop(\"token_type_ids\")\n",
    "output = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 6251,  102,    0],\n",
      "        [ 101, 2054, 2003, 1996, 6251, 2055,  102],\n",
      "        [ 101, 2748, 2009, 2003,  102,    0,    0]]), 'attention_mask': tensor([[1., 1., 1.]]), 'labels': tensor([[  101,  2023,  2003,  1996,  7978, 12654,   102],\n",
      "        [  101,  2023,  2003,  1996,  7978, 12654,   102],\n",
      "        [  101,  2023,  2003,  1996,  7978, 12654,   102]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = tokenizer(\"This is the corresponding summary\", return_tensors=\"pt\").input_ids\n",
    "labels = labels.expand(3,-1)\n",
    "tokens[\"labels\"] = labels.contiguous()\n",
    "print(tokens)\n",
    "orig_tokens = tokenizer.batch_encode_plus(dialogue, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[CLS] this is a sentence sentence. this is the sentence... this sentence is a. sentence. it is a crime. this. is a punishment. this was a sentence. that is a human this is. a sentence is.. that.. it. is. not a sentence,'this is an sentence. [SEP]\", '[CLS] the sentence is a sentence that is about the sentence. the sentence includes the sentence : \" i am not a racist. \" the sentence doesn\\'t mean it is about what is about. the sentences are the same as the sentence about what you are telling us. the words. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', \"[CLS] the'it is a no it'is the name of it. it is the most common it is. it's a very difficult thing it is,'it says. it ’ s the most important thing it'd do. it was the most difficult it is to do. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"]\n"
     ]
    }
   ],
   "source": [
    "orig_gen = orig_model.generate(**orig_tokens)\n",
    "print(tokenizer.batch_decode(orig_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] the incident took place in the town of kew, near the capital of south africa. the town is a popular destination for the local population. the city has a number of famous actors and actresses. the actor is now a celebrity in the uk. he is the latest in a series of celebrity celebrity - themed films. [SEP]',\n",
       " '[CLS] the incident took place in the town of kew, near the capital of south africa. the town is a popular destination for the local population. the city has a number of famous actors and actresses. the actor is now a celebrity in the uk. he is the latest in a series of celebrity celebrity - themed films. [SEP]',\n",
       " '[CLS] the incident took place in the town of kew, near the capital of south africa. the town is a popular destination for the local population. the city has a number of famous actors and actresses. the actor is now a celebrity in the uk. he is the latest in a series of celebrity celebrity - themed films. [SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdasdsad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-68234de5c63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdasdsad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasdsad' is not defined"
     ]
    }
   ],
   "source": [
    "asdasdsad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 391/391 [00:00<00:00, 95.5kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 105kB/s]\n",
      "Downloading: 100%|██████████| 3.95k/3.95k [00:00<00:00, 1.37MB/s]\n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 885B/s]\n",
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 221kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 34.0kB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 70.3kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [01:37<00:00, 4.49MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 25.8kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 55.3kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 986kB/s] \n",
      "Downloading: 100%|██████████| 399/399 [00:00<00:00, 231kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 542kB/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
